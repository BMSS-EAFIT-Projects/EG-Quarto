# Chapter 3: Random vectors

## Random variable

## Cumulative distribution function

## Joint distribution
*A particle can have a position or a velocity, but strictly speaking it cannot hace both... The more we clarify the secret of position, the more deeply the secret of velocity hides... We cna distribute uncertainty as we wish, but we can never eliminate it.*

### Joint Distribution Function
Throughout this unit we assume a probability sapce $(\Omega,\mathcal{F},P)$ correspondig to a given random experiment.

**Notation**. Given random variables
$$
\begin{align}
X \colon \Omega \longrightarrow
 \mathbb{R}
\qquad
X_1,\cdots, X_n \colon \Omega \longrightarrow
 \mathbb{R}
 \end{align}
$$

and subsets $B,B_1,\dots,B_n\subset\mathbb R$, we write:

$$
[ X \in B] \space \colon= \{\omega \in \Omega \colon X(\omega) \in B, \}
$$

$$
[X_1 \in B_1, \cdots, X_n \in B_n] \space \colon = \bigcap_{k=1}^{n}\{\omega \in \Omega \colon X_k(\omega) \in B_k\}
$$
and, for $A\subset\mathbb R^{n}$,

$$
[(X_1,\cdots,X_n) \in A] \space \colon = \{\omega \in \Omega \colon (X_1(\omega),\cdots,X_n(\omega))\in A\}
$$

Recall from the previous unit that all probabilistic information about a single random variable $X$ is contained in its distribution function: knowing it allows us to compute the probability of any event whose occurrence depends on the value taken by $X$.

Two random variables may be different as functions defined on the sample space $\Omega$, yet have the **same distribution**; from a probabilistic point of view they convey exactly the same information and can be used interchangeably for the same purpose.

**Example - Symmetry of the uniform $(0,1)$ distribution**
Consider the experiment of selecting a real number at random in the interval $(0,1)$. Let $X$ denote this quantity, i.e.

$$
X \sim Uniform(0,1).
$$
Define another random variable
$$
Y=1-X
$$

Although $X$ and $Y$ are different functions on the same sample space—indeed $X(x)=x$ and $Y(x)=1-x$—they share a remarkable property: they have the same distribution, because the continuous uniform distribution on $(0,1)$ is symmetric about $x=0.5$.

::: {.callout-note title="$Uniform(0,1)$"}
::: {.panel-tabset}
### R
```{r}
#| label: Symm-unif-r
#| code-fold: true

set.seed(123)           # Reproducibility
n <- 10000              # Number of simulations
x <- runif(n)           # X ~ Uniform(0,1)
y <- 1 - x              # Y = 1 - X

hist(x, breaks = 30, freq = FALSE,
     col = rgb(1,0,0,0.5), main = "Distribution of X and Y = 1 - X", xlab = "")
hist(y, breaks = 30, freq = FALSE,
     col = rgb(0,0,1,0.5), add = TRUE)
legend("topright", legend = c("X", "Y = 1 - X"),
       fill = c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)))
```
### Python
```{python}
#| label: sym-unif-py
#| code-fold: true

import numpy as np
import matplotlib.pyplot as plt

# Reproducibility
np.random.seed(123)

# Number of simulations
n = 10000

# X ~ Uniform(0,1)
x = np.random.uniform(0, 1, n)

# Y = 1 - X
y = 1 - x

# Histogram of X
plt.hist(x,
         bins=30,          # Number of bins
         density=True,     # relative frequency
         alpha=0.5,
         color='red',
         label='X')

# Histogram of Y
plt.hist(y,
         bins=30,
         density=True,
         alpha=0.5,
         color='blue',
         label='Y = 1 - X')

# 
plt.title('Distribution of X and Y = 1 - X')
plt.xlabel('Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()
```

:::
:::

The histogram almos perfectly overlap, illustrating that $X$ and $Y$ are different as functions but identically distributed:

$$
X \stackrel{d}{=} Y
$$

This example clarifies the distinction between *random variables* as *functions* and *probability distributions*.

**Example - Generating Exponential Variables from Uniforms**

Suppose we wish to generate $n$ realizations of an exponential variable $Y\sim \mathrm{Exp}(lambda=1)$ starting from uniforms.

*Method 1 - Classic inverse transform.* Generates $x_k\sim\mathrm{Uniform}(0,1)$, we may also take

$$
z_k = -ln(1-x_k), \space \space k=1,\cdots, n.
$$
Both transformations yield exponential samples with rate 1

::: {.callout-note title="Exponential transformation"}
::: {.panel-tabset}
### R
```{r}
#| label: exp-transform-r
#| code-fold: true

set.seed(42)
 n <- 10000
 x <- runif(n)
 y <- -log(x)
 z <- -log(1 - x)

hist(y, breaks = 50, freq = FALSE,
     col = rgb(1,0,0,0.4), main = "y = -log(x) vs z = -log(1 - x)")
hist(z, breaks = 50, freq = FALSE,
     col = rgb(0,0,1,0.4), add = TRUE)
curve(dexp(x, rate = 1), col = "black", lwd = 2, add = TRUE)
legend("topright", legend = c("y = -log(x)", "z = -log(1 - x)", "Exp(1) density"),
       fill = c(rgb(1,0,0,0.4), rgb(0,0,1,0.4), NA), border = NA,
       lty = c(NA, NA, 1), col = c(NA, NA, "black"))
```
### Python
```{python}
#| label: exp-transform-py
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt

# Reproducibility
np.random.seed(42)

# Number of simulations
n = 10_000

# X ~ Uniform(0,1)
x = np.random.uniform(0, 1, n)

# Transforms
y = -np.log(x)        # y = -log(x)
z = -np.log(1 - x)    # z = -log(1 - x)

# Histogram of y
plt.hist(y,
         bins=50,
         density=True,
         alpha=0.4,
         color='red',
         label='y = -log(x)')

# Histogram of z
plt.hist(z,
         bins=50,
         density=True,
         alpha=0.4,
         color='blue',
         label='z = -log(1 - x)')

# Theoric curve Exp(1): f(t) = e^{-t},  t ≥ 0
t = np.linspace(0, max(y.max(), z.max()), 400)
plt.plot(t, np.exp(-t), 'k-', linewidth=2, label='Exp(1) density')

# Graph
plt.title('y = -log(x) vs z = -log(1 - x)')
plt.xlabel('Value')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

```

:::
:::

Both histograms fit the theoretical density, showing that **multiple generators can produce the same distribution**.

When a problem involves several random variables, knowing their marginal distribution is **not enough**; we must undestand how they relate -That is, their *dependency structure*, captured by

- The join distribution;
- measure such as covariance or correlation;
- or more advanced tools such as copulas.

The next two examples illustrate independence and dependence

#### **Example - Random point inside a square (ndependent $X$ and $Y$)**

Select a point uniformly at random inside the unit square with vertices $A(0,0), B(1,0),C(1,1),D(0,1)$ and $Y$ be its abscissa and ordinate. The marginals are

$$
F_X(x) = \begin{cases}
0, & x \leq 0 \\
x, & 0 < x < 1 \\
1, & x \geq 1
\end{cases}
\quad\quad
F_Y(y) = \begin{cases}
0, & y \leq 0 \\
y, & 0 < y < 1 \\
1, & y \geq 1
\end{cases}
$$

Knowing $X$ gives no nformation about $Y$; hence $X$ and $Y$ are **independent**.

#### **Example 1.2 - Random point on the diagonal (Perfect dependence)**
Now select a point uniformly on the line $y=x$ within the same square. Again the marginals are uniform on $(0,1)$, but here $Y=X$, so the variables are **perfectly dependet**

| Characteristic | Example 1.1 | Example 1.2 |
|---|---|---|
| Selection region | Square area | Diagonal line |
| Relation between $X$ and $Y$ | Independent | Functional dependence $(X=Y)$ |
| Joint distribution | Product of marginals | Support only on $x=y$ |

**Joint Distribution of a family of random variables**

For $n$ random variables $X_1,\cdots X_n$, their hoint distribution is defined by

$$
F_{X_1,\cdots X_n}(x_1,\cdots x_n) = P[X_1\leq x_1,\cdots, X_n \leq x_n]
$$

it generalises the unvariate distribution function and contains both marginal information and dependence.

Some basic properties:
- it is non-decreasng and right-continuous in each argument.
- Taking limits to $+\infty$ in one coordinate recovers the corresponding marginal distribution.

### Joint Density Functions

Wheb studying a single random variable, two important cases are determined by a **densirty function**: the discrete case and the absolutely continuous case. The ideas extend naturally to vectors of randomvariables.

#### Discrete Bivariate Vector
A pair $(X,Y)$ is a **discrete random vector** if there exists a finite countable set of points $(x_m,y_m)$ such that

$$
P[X=x_m,Y=y_m] >0, \quad \sum_{m}P[X=x_m,Y=y_m]=1
$$
the **joint probability mass function** is then

$$
f_{X,Y}(x,y) = P[X=x,Y=y]
$$
Probabilities oer sets are computed via double sums; for rectangles this yields

$$
F_{X,Y}(x,y)=\sum_{u\leq x}\sum_{v\leq y}f_{X,Y}(u,v)
$$

**Example**
Let
$$
f_{X,Y}(x,y) = \begin{cases}
cx, & x,y \in {1,\cdots, N}\\
0,  & otw
\end{cases}
$$

Find:

- (a) $c$
- (b) $P[X=Y]$
- (c) $P[X<Y]$
- (d) $P[X>Y]$

*Solution*. Using $\sum_{x=1}^{N}\sum_{y=1}^{N} cx = 1$ we get

$$
c=\frac{2}{N^{2}(N+1)}
$$
Straightforward summations give

$$
P[X=Y]=\frac{1}{N}, \quad P[X<Y]=\frac{N-1}{3N}, \quad P[X>Y]=\frac{2(N-1)}{3N}
$$

*(R code for visualisation is included in the appendix)*

#### Multivariate Discrete Vector

Vector $(X_1, \cdots, X_n)$ taking countably many values $(x^{(m)}_1,\dots,x^{(m)}_n)$ are handled analogously, with joint pmf

$$
f_{X_1,\cdots,X_n}(x_1,\cdots,x_n)=P[X_1=x_1,\cdots,X_n=x_n]
$$


## Conditional densities

## Conditional espected value

## espected value

## Covariance and correlation

## Independence11

## Espected value vector and Covariance matrix

## Normal multivariate distribution and Multinomial distribution